apiVersion: template.openshift.io/v1
kind: Template
metadata:
  name: trino-template
  annotations:
    openshift.io/display-name: "Trino"
    openshift.io/long-description: "This template defines resources needed to deploy and run the Trino."
    openshift.io/provider-display-name: "Red Hat, Inc."
labels:
  app: trino
  template: trino
objects:
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: trino-scripts-${CONFIGMAP_HASH}
    labels:
      app: trino
  data:
    entrypoint.sh: |
      #!/bin/bash
      function importCert() {
        PEM_FILE=$1
        PASSWORD=$2
        KEYSTORE=$3
        # number of certs in the PEM file
        CERTS=$(grep 'END CERTIFICATE' $PEM_FILE| wc -l)
        # For every cert in the PEM file, extract it and import into the JKS keystore
        # awk command: step 1, if line is in the desired cert, print the line
        #              step 2, increment counter when last line of cert is found
        for N in $(seq 0 $(($CERTS - 1))); do
          ALIAS="${PEM_FILE%.*}-$N"
          cat $PEM_FILE |
            awk "n==$N { print }; /END CERTIFICATE/ { n++ }" |
            keytool -noprompt -import -trustcacerts \
                    -alias $ALIAS -keystore $KEYSTORE -storepass $PASSWORD
        done
      }
      set -e

      if [[ ! -z "${ACG_CONFIG}" ]]; then
        export DATABASE_HOST=$(jq -r '.database.hostname' ${ACG_CONFIG})
        export DATABASE_PORT=$(jq -r '.database.port' ${ACG_CONFIG})
        export DATABASE_USER=$(jq -r '.database.username' ${ACG_CONFIG})
        export DATABASE_PASSWORD=$(jq -r '.database.password' ${ACG_CONFIG})
        export DATABASE_NAME=$(jq -r '.database.name' ${ACG_CONFIG})
        export DATABASE_SSLMODE=$(jq -r '.database.sslMode' ${ACG_CONFIG})
        if [[ $DATABASE_SSLMODE = "null" ]]; then
          unset DATABASE_SSLMODE
        fi
        certString=$(jq -r '.database.rdsCa' ${ACG_CONFIG})
        if [[ $certString != "null" ]]; then
          temp_file=$(mktemp)
          echo "RDS Cert Path: $temp_file"
          echo "$certString" > $temp_file
          export PGSSLROOTCERT=$temp_file
        fi

        export AWS_ACCESS_KEY_ID=$(jq -r '.objectStore.buckets[0].accessKey' ${ACG_CONFIG})
        export AWS_SECRET_ACCESS_KEY=$(jq -r '.objectStore.buckets[0].secretKey' ${ACG_CONFIG})
        export S3_BUCKET_NAME=$(jq -r '.objectStore.buckets[0].requestedName' ${ACG_CONFIG})

        OBJECTSTORE_HOST=$(jq -r '.objectStore.hostname' ${ACG_CONFIG})
        OBJECTSTORE_PORT=$(jq -r '.objectStore.port' ${ACG_CONFIG})
        OBJECTSTORE_TLS=$(jq -r '.objectStore.tls' ${ACG_CONFIG})

        export URI_PREFIX=https
        if [[ $OBJECTSTORE_TLS == *"false"* ]]; then
          export URI_PREFIX=http
        fi

        S3_ENDPOINT="${URI_PREFIX}://${OBJECTSTORE_HOST}"
        if [[ -n "${OBJECTSTORE_PORT}" ]] && [[ "${OBJECTSTORE_PORT}" != "null" ]]; then
          S3_ENDPOINT="${S3_ENDPOINT}:${OBJECTSTORE_PORT}"
        fi
        export S3_ENDPOINT
      fi
      echo "Copy config files to ${TRINO_HOME}/"
      cp -v -L -r -f /etc/trino-init/* ${TRINO_HOME}/
      if [ ! -f ${TRINO_HOME}/catalog/postgres.properties ]; then
        echo "Creating trino connector configuration..."
        echo "connector.name=postgresql" >> ${TRINO_HOME}/catalog/postgres.properties
        echo "connection-url=jdbc:postgresql://${DATABASE_HOST}:${DATABASE_PORT}/${DATABASE_NAME}" >> ${TRINO_HOME}/catalog/postgres.properties
        echo "connection-user=${DATABASE_USER}" >> ${TRINO_HOME}/catalog/postgres.properties
        echo "connection-password=${DATABASE_PASSWORD}" >> ${TRINO_HOME}/catalog/postgres.properties
        echo "postgresql.array-mapping=AS_ARRAY" >> ${TRINO_HOME}/catalog/postgres.properties
        echo "insert.non-transactional-insert.enabled=${NON_TRANSACTIONAL_INSERT}" >> ${TRINO_HOME}/catalog/postgres.properties
        echo "write.batch-size=${INSERT_BATCH_SIZE}" >> ${TRINO_HOME}/catalog/postgres.properties

      fi
      # always add the openshift service-ca.crt if it exists
      if [ -a /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt ]; then
        echo "Adding /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt to $JAVA_HOME/lib/security/cacerts"
        importCert /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt changeit $JAVA_HOME/lib/security/cacerts
      fi
      # add node id to node config
      NODE_CONFIG="${TRINO_HOME}/node.properties"
      # ensure there's a newline between the last item in the config and what we add
      echo "" >> $NODE_CONFIG
      if ! grep -q -F 'node.id' "$NODE_CONFIG"; then
        NODE_ID="node.id=$MY_NODE_ID"
        echo "Adding $NODE_ID to $NODE_CONFIG"
        echo "$NODE_ID" >> "$NODE_CONFIG"
      fi
      CORE_SITE=/hadoop-config/core-site.xml
      mkdir -p ${TRINO_HOME}/hadoop-config
      if [[ -f ${CORE_SITE} ]]; then
        echo "Configuring core-site.xml"
        cat ${CORE_SITE} | sed "s#XXX_S3ENDPOINT_XXX#${S3_BUCKET_NAME}/${S3_DATA_DIR}#" > ${TRINO_HOME}/hadoop-config/core-site.xml
      fi
      # add AWS creds to hive catalog properties
      HIVE_CATALOG_CONFIG="${TRINO_HOME}/catalog/hive.properties"
      # ensure there's a newline between the last item in the config and what we add
      echo "" >> $HIVE_CATALOG_CONFIG
      if ! grep -q -F 'hive.s3.aws-access-key' "$HIVE_CATALOG_CONFIG"; then
        echo "Adding hive.s3.aws-access-key and hive.s3.aws-secret-key to $HIVE_CATALOG_CONFIG"
        echo "hive.s3.aws-access-key=$AWS_ACCESS_KEY_ID" >> "$HIVE_CATALOG_CONFIG"
        echo "hive.s3.aws-secret-key=$AWS_SECRET_ACCESS_KEY" >> "$HIVE_CATALOG_CONFIG"
        echo "hive.s3.endpoint=$S3_ENDPOINT" >> "$HIVE_CATALOG_CONFIG"
        echo "hive.s3.ssl.enabled=$OBJECTSTORE_TLS" >> "$HIVE_CATALOG_CONFIG"
      fi

      # add UID to /etc/passwd if missing
      if ! whoami &> /dev/null; then
          if test -w /etc/passwd || stat -c "%a" /etc/passwd | grep -qE '.[267].'; then
              echo "Adding user ${USER_NAME:-trino} with current UID $(id -u) to /etc/passwd"
              # Remove existing entry with user first.
              # cannot use sed -i because we do not have permission to write new
              # files into /etc
              sed  "/${USER_NAME:-trino}:x/d" /etc/passwd > /tmp/passwd
              # add our user with our current user ID into passwd
              echo "${USER_NAME:-trino}:x:$(id -u):0:${USER_NAME:-trino} user:${HOME}:/sbin/nologin" >> /tmp/passwd
              # overwrite existing contents with new contents (cannot replace the
              # file due to permissions)
              cat /tmp/passwd > /etc/passwd
              rm /tmp/passwd
              echo "Done adding user ${USER_NAME:-trino} with current UID $(id -u) to /etc/passwd"
          fi
      fi
      exec "$@"
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: trino-config-${CONFIGMAP_HASH}
    labels:
      app: trino
  data:
    access-control.properties: |-
      access-control.name=file
      security.config-file=/etc/trino/rules.json
    jvm.config: |-
      -server
      -Xmx${MAX_HEAP_SIZE}
      -XX:InitialRAMPercentage=80
      -XX:+UseContainerSupport
      -XX:+UseG1GC
      -XX:G1HeapRegionSize=32M
      -XX:G1ReservePercent=20
      -XX:+ExplicitGCInvokesConcurrent
      -XX:+ExitOnOutOfMemoryError
      -XX:ErrorFile=/data/trino/logs/java_error%p.log
      -XX:+HeapDumpOnOutOfMemoryError
      -XX:HeapDumpPath=/data/trino/logs/heap_dump.bin
      -XX:-OmitStackTraceInFastThrow
      -XX:ReservedCodeCacheSize=512M
      -XX:PerMethodRecompilationCutoff=10000
      -XX:PerBytecodeRecompilationCutoff=10000
      -XshowSettings:vm
      -Dfile.encoding=UTF-8
      -Djdk.attach.allowAttachSelf=true
      -Djdk.nio.maxCachedBufferSize=2000000
      -verbose:gc
      -Xlog:gc*:/data/trino/logs/gc.log
      -javaagent:/usr/lib/trino/jmx_exporter.jar=9000:/etc/trino/catalog/config.yaml
      # Reduce starvation of threads by GClocker, recommend to set about the number of cpu cores (JDK-8192647)
      -XX:+UnlockDiagnosticVMOptions
      -XX:GCLockerRetryAllocationCount=8
      # Allow loading dynamic agent used by JOL
      -XX:+EnableDynamicAgentLoading
      -XX:G1NumCollectionsKeepPinned=10000000
    log.properties: |-
      io.trino=INFO
    node.properties: |-
      node.data-dir=/data/trino/data
      node.environment=${NODE_ENV}
    config.properties.coordinator: |-
      coordinator=true
      spill-enabled=true
      spiller-spill-path=/data/trino/spill
      node-scheduler.include-coordinator=false
      http-server.http.port=10000
      discovery.uri=http://trino-coordinator:10000
      query.max-memory-per-node=${QUERY_MAX_MEMORY_PER_NODE}
      query.max-memory=${QUERY_MAX_MEMORY}
      query.max-total-memory=${QUERY_MAX_TOTAL_MEMORY}
      memory.heap-headroom-per-node=${MEMORY_HEAP_HEADROOM_PER_NODE}
      web-ui.authentication.type=fixed
      web-ui.user=trino
    config.properties.worker: |-
      coordinator=false
      spill-enabled=true
      spiller-spill-path=/data/trino/spill
      node-scheduler.include-coordinator=false
      http-server.http.port=10000
      discovery.uri=http://trino-coordinator:10000
      query.max-memory-per-node=${QUERY_MAX_MEMORY_PER_NODE}
      query.max-memory=${QUERY_MAX_MEMORY}
      query.max-total-memory=${QUERY_MAX_TOTAL_MEMORY}
      memory.heap-headroom-per-node=${MEMORY_HEAP_HEADROOM_PER_NODE}
    rules.json: |-
      {
        "catalogs": [
          {
            "user": "readonly",
            "catalog": "hive|postgres",
            "allow": "read-only"
          },
          {
            "user": "admin",
            "catalog": ".*",
            "allow": "all"
          }
        ],
        "procedures": [
          {
            "user": "admin",
            "privileges": [
              "EXECUTE",
              "GRANT_EXECUTE"
            ]
          }
        ],
        "system_session_properties": [
          {
            "user": "admin",
            "allow": true
          },
          {
            "user": "readonly",
            "allow": false
          }
        ],
        "catalog_session_properties": [
          {
            "user": "admin",
            "allow": true
          },
          {
            "user": "readonly",
            "allow": false
          }
        ],
        "system_information": [
          {
            "user": "admin",
            "allow": [
              "read",
              "write"
            ]
          },
          {
            "user": "readonly",
            "allow": [
              "read"
            ]
          }
        ]
      }
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: trino-config-catalog-${CONFIGMAP_HASH}
    labels:
      app: trino
  data:
    config.yaml: |-
      ---
      lowercaseOutputName: true
      lowercaseOutputLabelNames: true
      attrNameSnakeCase: false
      rules:
        # capture percentile and set quantile label
        - pattern: 'trino.plugin.hive<type=(.+), name=hive><>(.+AllTime).P(\d+): (.*)'
          name: 'trino_hive_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
          labels:
            quantile: '0.$3'
        # match non-percentiles
        - pattern: 'trino.plugin.hive<type=(.+), name=hive><>(.+AllTime.+): (.*)'
          name: 'trino_hive_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
          # counts
        - pattern: 'trino.plugin.hive<type=(.+), name=hive><>(.+TotalCount.*): (.*)'
          name: 'trino_hive_$1_$2_total'
          type: COUNTER
        # capture percentile and set quantile label
        - pattern: 'trino.plugin.hive.s3<type=(.+), name=hive><>(.+AllTime).P(\d+): (.*)'
          name: 'trino_hive_s3_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
          labels:
            quantile: '0.$3'
        # match non-percentiles
        - pattern: 'trino.plugin.hive.s3<type=(.+), name=hive><>(.+AllTime.+): (.*)'
          name: 'trino_hive_s3_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
          # counts
        - pattern: 'trino.plugin.hive.s3<type=(.+), name=hive><>(.+TotalCount.*): (.*)'
          name: 'trino_hive_s3_$1_$2_total'
          type: COUNTER
        # capture percentile and set quantile label
        - pattern: 'trino.plugin.hive.metastore.thrift<type=(.+), name=hive><>(.+AllTime).P(\d+): (.*)'
          name: 'trino_hive_metastore_thrift_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
          labels:
            quantile: '0.$3'
        # match non-percentiles
        - pattern: 'trino.plugin.hive.metastore.thrift<type=(.+), name=hive><>(.+AllTime.+): (.*)'
          name: 'trino_hive_metastore_thrift_$1_$2_count_seconds'
          type: GAUGE
          valueFactor: 0.001
        # counts
        - pattern: 'trino.plugin.hive.metastore.thrift<type=(.+), name=hive><>(.+TotalCount.*): (.*)'
          name: 'trino_hive_metastore_thrift_$1_$2_total'
          type: COUNTER
        - pattern: 'trino.execution<name=(.+)><>(.+AllTime).P(\d+): (.*)'
          name: 'trino_execution_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
          labels:
            quantile: '0.$3'
        - pattern: 'trino.execution<name=(.+)><>(.+AllTime.+): (.*)'
          name: 'trino_execution_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
        # counts
        - pattern: 'trino.execution<name=(.+)><>(.+TotalCount.*): (.*)'
          name: 'trino_execution_$1_$2_total'
          type: COUNTER
        - pattern: 'trino.memory<type=(.*), name=(.*)><>(.+): (.*)'
          name: 'trino_memory_$1_$2_$3'
          type: GAUGE
        - pattern: 'trino.failuredetector<name=HeartbeatFailureDetector><>ActiveCount: (.*)'
          name: 'trino_heartbeatdetector_activecount'
          type: GAUGE
    hive.properties: |-
      connector.name=hive
      hive.collect-column-statistics-on-write=true
      hive.compression-codec=SNAPPY
      hive.config.resources=/etc/trino/hadoop-config/core-site.xml
      hive.hdfs.authentication.type=NONE
      hive.metastore.authentication.type=NONE
      hive.metastore.thrift.client.connect-timeout=${METASTORE_TIMEOUT}
      hive.metastore.thrift.client.read-timeout=${METASTORE_READ_TIMEOUT}
      hive.max-partitions-per-scan=${HIVE_PARTITION_LIMIT}
      hive.partition-statistics-sample-size=${HIVE_PARTITION_STATS_SAMPLE_SIZE}
      hive.metastore.uri=thrift://hive-metastore:10000
      hive.parquet.use-column-names=true
      hive.s3.path-style-access=true
      hive.s3.sse.enabled=${S3_SSE_ENABLED}
      hive.storage-format=Parquet
    blackhole.propeties: |-
      connector.name=blackhole
    jmx.properties: |-
      connector.name=jmx
    memory.properties: |-
      connector.name=memory
    tpcds.properties: |-
      connector.name=tpcds
    tpch.properties: |-
      connector.name=tpch

- apiVersion: cloud.redhat.com/v1alpha1
  kind: ClowdApp
  metadata:
    name: trino
  spec:
    envName: ${ENV_NAME}
    deployments:
    - name: coordinator
      minReplicas: ${{COORDINATOR_REPLICAS}}
      webServices:
        public:
          enabled: false
        private:
          enabled: true
        metrics:
          enabled: true
      podSpec:
        metadata:
          annotations:
            ignore-check.kube-linter.io/minimum-three-replicas: This deployment uses 1 pod as currently the coordinator is a singleton
        image: ${IMAGE}:${IMAGE_TAG}
        command:
          - /etc/trino/scripts/entrypoint.sh
        args:
          - /usr/lib/trino/run-trino
          - --etc-dir=/etc/trino
          - --config=/etc/trino/config.properties.coordinator
        env:
          - name: MY_NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.uid
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: S3_DATA_DIR
            value: 'data'
          - name: QUERY_MAX_MEMORY_PER_NODE
            value: ${QUERY_MAX_MEMORY_PER_NODE}
          - name: QUERY_MAX_MEMORY
            value: ${QUERY_MAX_MEMORY}
          - name: QUERY_MAX_TOTAL_MEMORY
            value: ${QUERY_MAX_TOTAL_MEMORY}
          - name: MEMORY_HEAP_HEADROOM_PER_NODE
            value: ${MEMORY_HEAP_HEADROOM_PER_NODE}
          - name: MAX_HEAP_SIZE
            value: ${MAX_HEAP_SIZE}
          - name: TRINO_HISTORY_FILE
            value: ${TRINO_HISTORY_FILE}
        machinePool: ${MACHINE_POOL_OPTION}
        resources:
          limits:
            cpu: ${CPU_LIMIT_COORDINATOR}
            memory: ${MEMORY_LIMIT_COORDINATOR}
          requests:
            cpu: ${CPU_REQUEST_COORDINATOR}
            memory: ${MEMORY_REQUEST_COORDINATOR}
        livenessProbe:
          httpGet:
            path: /v1/info
            port: private
          initialDelaySeconds: 60
          periodSeconds: ${{LIVENESS_PROBE_PERIOD}}
          successThreshold: 1
          failureThreshold: 6
          timeoutSeconds: ${{LIVENESS_PROBE_TIMEOUT}}
        readinessProbe:
          httpGet:
            path: /v1/info
            port: private
          initialDelaySeconds: 60
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 6
          timeoutSeconds: 5
        volumes:
          - name: trino-scripts
            configMap:
              name: trino-scripts-${CONFIGMAP_HASH}
              items:
              - key: entrypoint.sh
                path: entrypoint.sh
                mode: 509
          - name: trino-config
            configMap:
              name: trino-config-${CONFIGMAP_HASH}
          - name: trino-config-catalog
            configMap:
              name: trino-config-catalog-${CONFIGMAP_HASH}
          - name: trino-etc
            emptyDir: {}
          - name: trino-data
            emptyDir: {}
          - name: trino-logs
            emptyDir: {}
          - name: trino-spill
            emptyDir: {}
          - name: hadoop-config
            configMap:
              name: hadoop-clowder-config
        volumeMounts:
          - name: trino-etc
            mountPath: /etc/trino
          - name: trino-scripts
            mountPath: /etc/trino/scripts
          - name: trino-config
            mountPath: /etc/trino-init
          - name: trino-config-catalog
            mountPath: /etc/trino-init/catalog
          - name: trino-data
            mountPath: /data/trino/data
          - name: trino-logs
            mountPath: /data/trino/logs
          - name: trino-spill
            mountPath: /data/trino/spill
          - name: hadoop-config
            mountPath: /etc/trino/hadoop-config
    - name: worker
      minReplicas: ${{WORKER_REPLICAS}}
      webServices:
        public:
          enabled: false
        private:
          enabled: true
        metrics:
          enabled: true
      podSpec:
        image: ${IMAGE}:${IMAGE_TAG}
        command:
          - /etc/trino/scripts/entrypoint.sh
        args:
          - /usr/lib/trino/run-trino
          - --etc-dir=/etc/trino
          - --config=/etc/trino/config.properties.worker
        env:
          - name: MY_NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.uid
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: S3_DATA_DIR
            value: 'data'
          - name: QUERY_MAX_MEMORY_PER_NODE
            value: ${QUERY_MAX_MEMORY_PER_NODE}
          - name: QUERY_MAX_MEMORY
            value: ${QUERY_MAX_MEMORY}
          - name: QUERY_MAX_TOTAL_MEMORY
            value: ${QUERY_MAX_TOTAL_MEMORY}
          - name: MEMORY_HEAP_HEADROOM_PER_NODE
            value: ${MEMORY_HEAP_HEADROOM_PER_NODE}
          - name: MAX_HEAP_SIZE
            value: ${MAX_HEAP_SIZE}
          - name: TRINO_HISTORY_FILE
            value: ${TRINO_HISTORY_FILE}
        machinePool: ${MACHINE_POOL_OPTION}
        resources:
          limits:
            cpu: ${CPU_LIMIT_WORKER}
            memory: ${MEMORY_LIMIT_WORKER}
          requests:
            cpu: ${CPU_REQUEST_WORKER}
            memory: ${MEMORY_REQUEST_WORKER}
        livenessProbe:
          httpGet:
            path: /v1/info
            port: private
          initialDelaySeconds: 60
          periodSeconds: ${{LIVENESS_PROBE_PERIOD}}
          successThreshold: 1
          failureThreshold: 6
          timeoutSeconds: ${{LIVENESS_PROBE_TIMEOUT}}
        readinessProbe:
          httpGet:
            path: /v1/info
            port: private
          initialDelaySeconds: 60
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 6
          timeoutSeconds: 5
        terminationGracePeriodSeconds: 3600
        lifecycle:
          preStop:
            exec:
              command:
                - /usr/lib/trino/warm-shutdown
        volumes:
          - name: trino-scripts
            configMap:
              name: trino-scripts-${CONFIGMAP_HASH}
              items:
              - key: entrypoint.sh
                path: entrypoint.sh
                mode: 509
          - name: trino-config
            configMap:
              name: trino-config-${CONFIGMAP_HASH}
          - name: trino-config-catalog
            configMap:
              name: trino-config-catalog-${CONFIGMAP_HASH}
          - name: trino-etc
            emptyDir: {}
          - name: trino-data
            emptyDir: {}
          - name: trino-logs
            emptyDir: {}
          - name: trino-spill
            emptyDir: {}
          - name: hadoop-config
            configMap:
              name: hadoop-clowder-config
        volumeMounts:
          - name: trino-etc
            mountPath: /etc/trino
          - name: trino-scripts
            mountPath: /etc/trino/scripts
          - name: trino-config
            mountPath: /etc/trino-init
          - name: trino-config-catalog
            mountPath: /etc/trino-init/catalog
          - name: trino-data
            mountPath: /data/trino/data
          - name: trino-logs
            mountPath: /data/trino/logs
          - name: trino-spill
            mountPath: /data/trino/spill
          - name: hadoop-config
            mountPath: /etc/trino/hadoop-config
    objectStore:
    - ${S3_BUCKET_NAME}
    database:
      sharedDbAppName: koku
    dependencies:
      - koku

parameters:
# Clowdapp Params
- description: Image name
  name: IMAGE
  value: quay.io/cloudservices/ubi-trino
  required: true
- description: Image tag
  displayName: Image tag
  name: IMAGE_TAG
  value: '368-001'
  required: true
- name: ENV_NAME
  required: false
- name: S3_BUCKET_NAME
  value: 'hccm-s3'
- name: NODE_ENV
  value: 'production'
- name: MACHINE_POOL_OPTION
  value: ''
- name: TRINO_HISTORY_FILE
  value: /data/trino/data/.trino_history

# Coordinator Params
- description: Number of replicas for the coordinator
  displayName: Coordinator replica count
  name: COORDINATOR_REPLICAS
  required: true
  value: '1'
- description: Initial amount of memory the Django container will request.
  displayName: Memory Request
  name: MEMORY_REQUEST_COORDINATOR
  required: true
  value: 2Gi
- description: Maximum amount of memory the Django container can use.
  displayName: Memory Limit
  name: MEMORY_LIMIT_COORDINATOR
  required: true
  value: 4Gi
- description: Initial amount of cpu the Django container will request.
  displayName: CPU Request
  name: CPU_REQUEST_COORDINATOR
  required: true
  value: 250m
- description: Maximum amount of cpu the Django container can use.
  displayName: CPU Limit
  name: CPU_LIMIT_COORDINATOR
  required: true
  value: 500m

# Worker Params
- description: Number of replicas for the worker
  displayName: Worker replica count
  name: WORKER_REPLICAS
  required: true
  value: '2'
- description: Initial amount of memory the Django container will request.
  displayName: Memory Request
  name: MEMORY_REQUEST_WORKER
  required: true
  value: 2Gi
- description: Maximum amount of memory the Django container can use.
  displayName: Memory Limit
  name: MEMORY_LIMIT_WORKER
  required: true
  value: 4Gi
- description: Initial amount of cpu the Django container will request.
  displayName: CPU Request
  name: CPU_REQUEST_WORKER
  required: true
  value: 250m
- description: Maximum amount of cpu the Django container can use.
  displayName: CPU Limit
  name: CPU_LIMIT_WORKER
  required: true
  value: 500m

# JVM Params
- description: maximum heap size
  displayName: xmx
  name: MAX_HEAP_SIZE
  value: '3G'

# Trino configruation Params
- description: Max amount of user memory a query can use on a worker (Trino default - JVM max memory * 0.3)
  displayName: query.max-memory-per-node
  name: QUERY_MAX_MEMORY_PER_NODE
  value: '2GB'
  required: true
- description: Max amount of user memory a query can use across the entire cluster (Trino default - 20GB)
  displayName: query.max-memory
  name: QUERY_MAX_MEMORY
  value: '4GB'
  required: true
- description: Max amount of memory a query can use across the entire cluster, including revocable memory (Trino default - query.max-memory * 2)
  displayName: query.max-total-memory
  name: QUERY_MAX_TOTAL_MEMORY
  value: '8GB'
  required: true
- description: Amount of memory set aside as headroom/buffer in the JVM heap for allocations that are not tracked by Trino (Trino default - JVM max memory * 0.3)
  displayName: memory.heap-headroom-per-node
  name: MEMORY_HEAP_HEADROOM_PER_NODE
  value: '1GB'
  required: true

# Trino Hive config
- description: Socket connect timeout for metastore client
  displayName: hive.metastore.thrift.client.connect-timeout
  name: METASTORE_READ_TIMEOUT
  value: '300s'
- description: Socket read timeout for metastore client
  displayName: hive.metastore.thrift.client.read-timeout
  name: METASTORE_TIMEOUT
  value: '300s'
- description: Specifies the number of partitions to analyze when computing table statistics
  displayName: hive.partition-statistics-sample-size
  name: HIVE_PARTITION_STATS_SAMPLE_SIZE
  value: '100'
- description: Maximum number of partitions for a single table scan.
  displayName: hive.max-partitions-per-scan
  name: HIVE_PARTITION_LIMIT
  value: '1000000'
- description: Whether to use SSE encryption
  displayName: hive.s3.sse.enabled
  name: S3_SSE_ENABLED
  value: 'True'

# Trino Postgres config
- description: Number of rows to insert into Postgres.
  displayName: write.batch-size
  name: INSERT_BATCH_SIZE
  value: '10000'
- description: Whether to use non transactional inserts
  displayName: insert.non-transactional-insert.enabled
  name: NON_TRANSACTIONAL_INSERT
  value: 'True'


# Probe params
- description: Liveness probe timeout
  displayName: livenessTimeoutSeconds
  name: LIVENESS_PROBE_TIMEOUT
  value: '120'
- description: Liveness probe period
  displayName: livenessPeriodSeconds
  name: LIVENESS_PROBE_PERIOD
  value: '120'

# Configmap updater
- name: CONFIGMAP_HASH
  description: "The random hash to change the configmap names"
  value: '000001'
