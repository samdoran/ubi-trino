apiVersion: v1
kind: Template
metadata:
  name: presto-template
  annotations:
    openshift.io/display-name: "Presto"
    openshift.io/long-description: "This template defines resources needed to deploy and run the Presto."
    openshift.io/provider-display-name: "Red Hat, Inc."
labels:
  app: presto
  template: presto
objects:
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: presto-clowder-common-config
  data:
    initialize_presto.sh: |
      #!/bin/bash
      set -e
      if [[ ! -z "${ACG_CONFIG}" ]]; then
        export DATABASE_HOST=$(jq -r '.database.hostname' ${ACG_CONFIG})
        export DATABASE_PORT=$(jq -r '.database.port' ${ACG_CONFIG})
        export DATABASE_USER=$(jq -r '.database.username' ${ACG_CONFIG})
        export DATABASE_PASSWORD=$(jq -r '.database.password' ${ACG_CONFIG})
        export DATABASE_NAME=$(jq -r '.database.name' ${ACG_CONFIG})
        export DATABASE_SSLMODE=$(jq -r '.database.sslMode' ${ACG_CONFIG})
        if [[ $DATABASE_SSLMODE = "null" ]]; then
          unset DATABASE_SSLMODE
        fi
        certString=$(jq -r '.database.rdsCa' ${ACG_CONFIG})
        if [[ $certString != "null" ]]; then
          temp_file=$(mktemp)
          echo "RDS Cert Path: $temp_file"
          echo "$certString" > $temp_file
          export PGSSLROOTCERT=$temp_file
        fi
      fi
      echo "Copy config files to /opt/presto/presto-server/etc/"
      cp -v -L -r -f /presto-etc/* /opt/presto/presto-server/etc/
      if [ ! -f /opt/presto/presto-server/etc/catalog/postgres.properties ]; then
        echo "Creating presto connector configuration..."
        echo "connector.name=postgresql" >> /opt/presto/presto-server/etc/catalog/postgres.properties
        echo "connection-url=jdbc:postgresql://${DATABASE_HOST}:${DATABASE_PORT}/${DATABASE_NAME}" >> /opt/presto/presto-server/etc/catalog/postgres.properties
        echo "connection-user=${DATABASE_USER}" >> /opt/presto/presto-server/etc/catalog/postgres.properties
        echo "connection-password=${DATABASE_PASSWORD}" >> /opt/presto/presto-server/etc/catalog/postgres.properties
        echo "postgresql.array-mapping=AS_ARRAY" >> /opt/presto/presto-server/etc/catalog/postgres.properties
      fi
    entrypoint.sh: |
      #!/bin/bash
      function importCert() {
        PEM_FILE=$1
        PASSWORD=$2
        KEYSTORE=$3
        # number of certs in the PEM file
        CERTS=$(grep 'END CERTIFICATE' $PEM_FILE| wc -l)
        # For every cert in the PEM file, extract it and import into the JKS keystore
        # awk command: step 1, if line is in the desired cert, print the line
        #              step 2, increment counter when last line of cert is found
        for N in $(seq 0 $(($CERTS - 1))); do
          ALIAS="${PEM_FILE%.*}-$N"
          cat $PEM_FILE |
            awk "n==$N { print }; /END CERTIFICATE/ { n++ }" |
            keytool -noprompt -import -trustcacerts \
                    -alias $ALIAS -keystore $KEYSTORE -storepass $PASSWORD
        done
      }
      set -e

      if [[ ! -z "${ACG_CONFIG}" ]]; then
        export AWS_ACCESS_KEY_ID=$(jq -r '.objectStore.buckets[0].accessKey' ${ACG_CONFIG})
        export AWS_SECRET_ACCESS_KEY=$(jq -r '.objectStore.buckets[0].secretKey' ${ACG_CONFIG})
        export S3_BUCKET_NAME=$(jq -r '.objectStore.buckets[0].requestedName' ${ACG_CONFIG})

        OBJECTSTORE_HOST=$(jq -r '.objectStore.hostname' ${ACG_CONFIG})
        OBJECTSTORE_PORT=$(jq -r '.objectStore.port' ${ACG_CONFIG})
        OBJECTSTORE_TLS=$(jq -r '.objectStore.tls' ${ACG_CONFIG})

        export URI_PREFIX=https
        if [[ $OBJECTSTORE_TLS == *"false"* ]]; then
          export URI_PREFIX=http
        fi

        S3_ENDPOINT="${URI_PREFIX}://${OBJECTSTORE_HOST}"
        if [[ -n "${OBJECTSTORE_PORT}" ]] && [[ "${OBJECTSTORE_PORT}" != "null" ]]; then
          S3_ENDPOINT="${S3_ENDPOINT}:${OBJECTSTORE_PORT}"
        fi
        export S3_ENDPOINT
      fi
      # always add the openshift service-ca.crt if it exists
      if [ -a /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt ]; then
        echo "Adding /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt to $JAVA_HOME/lib/security/cacerts"
        importCert /var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt changeit $JAVA_HOME/lib/security/cacerts
      fi
      # add node id to node config
      NODE_CONFIG="${PRESTO_HOME}/etc/node.properties"
      # ensure there's a newline between the last item in the config and what we add
      echo "" >> $NODE_CONFIG
      if ! grep -q -F 'node.id' "$NODE_CONFIG"; then
        NODE_ID="node.id=$MY_NODE_ID"
        echo "Adding $NODE_ID to $NODE_CONFIG"
        echo "$NODE_ID" >> "$NODE_CONFIG"
      fi
      CORE_SITE=/hadoop-config/core-site.xml
      mkdir -p ${PRESTO_HOME}/etc/hadoop
      if [[ -f ${CORE_SITE} ]]; then
        echo "Configuring core-site.xml"
        cat ${CORE_SITE} | sed "s#XXX_S3ENDPOINT_XXX#${S3_BUCKET_NAME}/${S3_DATA_DIR}#" > ${PRESTO_HOME}/etc/hadoop/core-site.xml
      fi
      # add AWS creds to hive catalog properties
      HIVE_CATALOG_CONFIG="${PRESTO_HOME}/etc/catalog/hive.properties"
      # ensure there's a newline between the last item in the config and what we add
      echo "" >> $HIVE_CATALOG_CONFIG
      if ! grep -q -F 'hive.s3.aws-access-key' "$HIVE_CATALOG_CONFIG"; then
        echo "Adding hive.s3.aws-access-key and hive.s3.aws-secret-key to $HIVE_CATALOG_CONFIG"
        echo "hive.s3.aws-access-key=$AWS_ACCESS_KEY_ID" >> "$HIVE_CATALOG_CONFIG"
        echo "hive.s3.aws-secret-key=$AWS_SECRET_ACCESS_KEY" >> "$HIVE_CATALOG_CONFIG"
        echo "hive.s3.endpoint=$S3_ENDPOINT" >> "$HIVE_CATALOG_CONFIG"
        echo "hive.s3.ssl.enabled=$OBJECTSTORE_TLS" >> "$HIVE_CATALOG_CONFIG"
      fi
      # add max memory config to config.properties
      CONFIG="${PRESTO_HOME}/etc/config.properties"
      # ensure there's a newline between the last item in the config and what we add
      echo "" >> $CONFIG
      if ! grep -q -F 'query.max-memory-per-node' "$CONFIG"; then
        echo "Adding query.max-memory, query.max-memory-per-node, and query.max-total-memory-per-node to $CONFIG"
        echo "query.max-memory=$QUERY_MAX_MEMORY" >> "$CONFIG"
        echo "query.max-memory-per-node=$QUERY_MAX_MEMORY_PER_NODE" >> "$CONFIG"
        echo "query.max-total-memory-per-node=$QUERY_MAX_TOTAL_MEMORY_PER_NODE" >> "$CONFIG"
      fi
      # add UID to /etc/passwd if missing
      if ! whoami &> /dev/null; then
          if test -w /etc/passwd || stat -c "%a" /etc/passwd | grep -qE '.[267].'; then
              echo "Adding user ${USER_NAME:-presto} with current UID $(id -u) to /etc/passwd"
              # Remove existing entry with user first.
              # cannot use sed -i because we do not have permission to write new
              # files into /etc
              sed  "/${USER_NAME:-presto}:x/d" /etc/passwd > /tmp/passwd
              # add our user with our current user ID into passwd
              echo "${USER_NAME:-presto}:x:$(id -u):0:${USER_NAME:-presto} user:${HOME}:/sbin/nologin" >> /tmp/passwd
              # overwrite existing contents with new contents (cannot replace the
              # file due to permissions)
              cat /tmp/passwd > /etc/passwd
              rm /tmp/passwd
          fi
      fi
      exec "$@"
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: presto-clowder-coordinator-config
    labels:
      app: presto
  data:
    config.properties: |
      coordinator=true
      node-scheduler.include-coordinator=true
      discovery-server.enabled=true
      http-server.http.port=8000
      discovery.uri=http://presto-coordinator:8000
      jmx.rmiserver.port=8081
      jmx.rmiregistry.port=8081
      query.max-length=10000000
    jvm.config: >-
      -server

      -XX:+UseContainerSupport

      -XX:+UseG1GC

      -XX:+UseGCOverheadLimit

      -XX:InitialRAMPercentage=50.0

      -XX:MaxRAMPercentage=75.0

      -XX:+ExplicitGCInvokesConcurrent

      -XX:+HeapDumpOnOutOfMemoryError

      -XX:HeapDumpPath=/var/presto/logs/heap_dump.bin

      -XX:+ExitOnOutOfMemoryError

      -XX:ErrorFile=/var/presto/logs/java_error%p.log

      -verbose:gc

      -Xlog:gc*:/var/presto/logs/gc.log

      -javaagent:/opt/jmx_exporter/jmx_exporter.jar=9000:/opt/jmx_exporter/config/config.yml

      -Dcom.sun.management.jmxremote

      -Dcom.sun.management.jmxremote.local.only=false

      -Dcom.sun.management.jmxremote.ssl=false

      -Dcom.sun.management.jmxremote.authenticate=false

      -Dcom.sun.management.jmxremote.port=8081

      -Dcom.sun.management.jmxremote.rmi.port=8081

      -Djava.rmi.server.hostname=127.0.0.1
    log.properties: |
      com.facebook.presto=INFO
    node.properties: |
      node.data-dir=/var/presto/data
      node.environment=production
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: presto-clowder-jmx-config
    labels:
      app: presto
  data:
    config.yml: |-
      ---
      lowercaseOutputName: true
      lowercaseOutputLabelNames: true
      attrNameSnakeCase: false
      rules:
        # capture percentile and set quantile label
        - pattern: 'presto.plugin.hive<type=(.+), name=hive><>(.+AllTime).P(\d+): (.*)'
          name: 'presto_hive_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
          labels:
            quantile: '0.$3'
        # match non-percentiles
        - pattern: 'presto.plugin.hive<type=(.+), name=hive><>(.+AllTime.+): (.*)'
          name: 'presto_hive_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
          # counts
        - pattern: 'presto.plugin.hive<type=(.+), name=hive><>(.+TotalCount.*): (.*)'
          name: 'presto_hive_$1_$2_total'
          type: COUNTER
        # capture percentile and set quantile label
        - pattern: 'presto.plugin.hive.s3<type=(.+), name=hive><>(.+AllTime).P(\d+): (.*)'
          name: 'presto_hive_s3_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
          labels:
            quantile: '0.$3'
        # match non-percentiles
        - pattern: 'presto.plugin.hive.s3<type=(.+), name=hive><>(.+AllTime.+): (.*)'
          name: 'presto_hive_s3_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
          # counts
        - pattern: 'presto.plugin.hive.s3<type=(.+), name=hive><>(.+TotalCount.*): (.*)'
          name: 'presto_hive_s3_$1_$2_total'
          type: COUNTER
        # capture percentile and set quantile label
        - pattern: 'presto.plugin.hive.metastore.thrift<type=(.+), name=hive><>(.+AllTime).P(\d+): (.*)'
          name: 'presto_hive_metastore_thrift_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
          labels:
            quantile: '0.$3'
        # match non-percentiles
        - pattern: 'presto.plugin.hive.metastore.thrift<type=(.+), name=hive><>(.+AllTime.+): (.*)'
          name: 'presto_hive_metastore_thrift_$1_$2_count_seconds'
          type: GAUGE
          valueFactor: 0.001
        # counts
        - pattern: 'presto.plugin.hive.metastore.thrift<type=(.+), name=hive><>(.+TotalCount.*): (.*)'
          name: 'presto_hive_metastore_thrift_$1_$2_total'
          type: COUNTER
        - pattern: 'presto.execution<name=(.+)><>(.+AllTime).P(\d+): (.*)'
          name: 'presto_execution_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
          labels:
            quantile: '0.$3'
        - pattern: 'presto.execution<name=(.+)><>(.+AllTime.+): (.*)'
          name: 'presto_execution_$1_$2_seconds'
          type: GAUGE
          valueFactor: 0.001
        # counts
        - pattern: 'presto.execution<name=(.+)><>(.+TotalCount.*): (.*)'
          name: 'presto_execution_$1_$2_total'
          type: COUNTER
        - pattern: 'presto.memory<type=(.*), name=(.*)><>(.+): (.*)'
          name: 'presto_memory_$1_$2_$3'
          type: GAUGE
        - pattern: 'presto.failuredetector<name=HeartbeatFailureDetector><>ActiveCount: (.*)'
          name: 'presto_heartbeatdetector_activecount'
          type: GAUGE
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: presto-clowder-worker-config
    labels:
      app: presto
  data:
    config.properties: |
      coordinator=false
      node-scheduler.include-coordinator=true
      http-server.http.port=8000
      discovery.uri=http://presto-coordinator:8000
      jmx.rmiserver.port=8081
      jmx.rmiregistry.port=8081
      query.max-length=10000000
    jvm.config: >-
      -server

      -XX:+UseContainerSupport

      -XX:+UseG1GC

      -XX:+UseGCOverheadLimit

      -XX:InitialRAMPercentage=50.0

      -XX:MaxRAMPercentage=75.0

      -XX:+ExplicitGCInvokesConcurrent

      -XX:+HeapDumpOnOutOfMemoryError

      -XX:HeapDumpPath=/var/presto/logs/heap_dump.bin

      -XX:+ExitOnOutOfMemoryError

      -XX:ErrorFile=/var/presto/logs/java_error%p.log

      -verbose:gc

      -Xlog:gc*:/var/presto/logs/gc.log

      -javaagent:/opt/jmx_exporter/jmx_exporter.jar=9000:/opt/jmx_exporter/config/config.yml

      -Dcom.sun.management.jmxremote

      -Dcom.sun.management.jmxremote.local.only=false

      -Dcom.sun.management.jmxremote.ssl=false

      -Dcom.sun.management.jmxremote.authenticate=false

      -Dcom.sun.management.jmxremote.port=8081

      -Dcom.sun.management.jmxremote.rmi.port=8081

      -Djava.rmi.server.hostname=127.0.0.1
    log.properties: |
      com.facebook.presto=INFO
    node.properties: |
      node.data-dir=/var/presto/data
      node.environment=production
- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: presto-clowder-catalog-config
    labels:
      app: presto
  data:
    hive.properties: |
      connector.name=hive-hadoop2
      hive.allow-drop-table=true
      hive.allow-rename-table=true
      hive.collect-column-statistics-on-write=true
      hive.compression-codec=SNAPPY
      hive.config.resources=/opt/presto/presto-server/etc/hadoop/core-site.xml
      hive.hdfs.authentication.type=NONE
      hive.metastore.authentication.type=NONE
      hive.metastore.uri=thrift://hive-metastore:8000
      hive.parquet.use-column-names=true
      hive.s3.path-style-access=true
      hive.s3.sse.enabled=true
      hive.storage-format=Parquet
    blackhole.propeties: |
      connector.name=blackhole
    jmx.properties: |
      connector.name=jmx
    memory.properties: |
      connector.name=memory
    tpcds.properties: |
      connector.name=tpcds
    tpch.properties: |
      connector.name=tpch

- apiVersion: cloud.redhat.com/v1alpha1
  kind: ClowdApp
  metadata:
    name: presto
  spec:
    envName: ${ENV_NAME}
    deployments:
    - name: coordinator
      minReplicas: ${{COORDINATOR_REPLICAS}}
      webServices:
        public:
          enabled: true
        metrics:
          enabled: true
      podSpec:
        image: ${IMAGE}:${IMAGE_TAG}
        initContainers:
          - command:
              - /presto-common/initialize_presto.sh
            inheritEnv: true
        command:
          - /presto-common/entrypoint.sh
        args:
          - /opt/presto/presto-server/bin/launcher
          - run
        env:
          - name: MY_NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.uid
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: S3_DATA_DIR
            value: 'data'
          - name: QUERY_MAX_MEMORY
            value: ${QUERY_MAX_MEMORY}
          - name: QUERY_MAX_MEMORY_PER_NODE
            value: ${QUERY_MAX_MEMORY_PER_NODE}
          - name: QUERY_MAX_TOTAL_MEMORY_PER_NODE
            value: ${QUERY_MAX_TOTAL_MEMORY_PER_NODE}
        resources:
          limits:
            cpu: ${COORDINATOR_CPU_LIMIT}
            memory: ${COORDINATOR_MEMORY_LIMIT}
          requests:
            cpu: ${COORDINATOR_CPU_REQUEST}
            memory: ${COORDINATOR_MEMORY_REQUEST}
        livenessProbe:
          failureThreshold: 3
          tcpSocket:
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          httpGet:
            path: /ui/
            port: 8000
            scheme: HTTP
          initialDelaySeconds: 120
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 3
          timeoutSeconds: 5
        volumes:
          - name: presto-coordinator-config
            configMap:
              name: presto-clowder-coordinator-config
              defaultMode: 420
          - name: presto-common-config
            configMap:
              name: presto-clowder-common-config
              defaultMode: 509
          - name: presto-catalog-config
            configMap:
              name: presto-clowder-catalog-config
              defaultMode: 420
          - name: presto-jmx-config
            configMap:
              name: presto-clowder-jmx-config
              defaultMode: 420
          - name: presto-etc
            emptyDir: {}
          - name: presto-data
            emptyDir: {}
          - name: presto-logs
            emptyDir: {}
          - name: hadoop-config
            configMap:
              name: hadoop-clowder-config
              defaultMode: 420
        volumeMounts:
          - name: presto-etc
            mountPath: /opt/presto/presto-server/etc
          - name: presto-coordinator-config
            mountPath: /presto-etc
          - name: presto-common-config
            mountPath: /presto-common
          - name: presto-jmx-config
            mountPath: /opt/jmx_exporter/config
          - name: presto-catalog-config
            mountPath: /presto-etc/catalog
          - name: presto-data
            mountPath: /var/presto/data
          - name: presto-logs
            mountPath: /var/presto/logs
          - name: hadoop-config
            mountPath: /hadoop-config
    - name: worker
      minReplicas: ${{WORKER_REPLICAS}}
      webServices:
        public:
          enabled: true
        metrics:
          enabled: true
      podSpec:
        image: ${IMAGE}:${IMAGE_TAG}
        initContainers:
          - command:
              - /presto-common/initialize_presto.sh
            inheritEnv: true
        command:
          - /presto-common/entrypoint.sh
        args:
          - /opt/presto/presto-server/bin/launcher
          - run
        env:
          - name: MY_NODE_ID
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.uid
          - name: MY_NODE_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: spec.nodeName
          - name: MY_POD_NAME
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.name
          - name: MY_POD_NAMESPACE
            valueFrom:
              fieldRef:
                apiVersion: v1
                fieldPath: metadata.namespace
          - name: S3_DATA_DIR
            value: 'data'
          - name: QUERY_MAX_MEMORY
            value: ${QUERY_MAX_MEMORY}
          - name: QUERY_MAX_MEMORY_PER_NODE
            value: ${QUERY_MAX_MEMORY_PER_NODE}
          - name: QUERY_MAX_TOTAL_MEMORY_PER_NODE
            value: ${QUERY_MAX_TOTAL_MEMORY_PER_NODE}
        resources:
          limits:
            cpu: ${WORKER_CPU_LIMIT}
            memory: ${WORKER_MEMORY_LIMIT}
          requests:
            cpu: ${WORKER_CPU_REQUEST}
            memory: ${WORKER_MEMORY_REQUEST}
        livenessProbe:
          failureThreshold: 3
          tcpSocket:
            port: 8000
          initialDelaySeconds: 120
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 3
        readinessProbe:
          httpGet:
            path: /
            port: 9000
            scheme: HTTP
          initialDelaySeconds: 120
          periodSeconds: 10
          successThreshold: 1
          failureThreshold: 3
          timeoutSeconds: 3
        volumes:
          - name: presto-worker-config
            configMap:
              name: presto-clowder-worker-config
              defaultMode: 420
          - name: presto-common-config
            configMap:
              name: presto-clowder-common-config
              defaultMode: 509
          - name: presto-catalog-config
            configMap:
              name: presto-clowder-catalog-config
              defaultMode: 420
          - name: presto-jmx-config
            configMap:
              name: presto-clowder-jmx-config
              defaultMode: 420
          - name: presto-etc
            emptyDir: {}
          - name: presto-data
            emptyDir: {}
          - name: presto-logs
            emptyDir: {}
          - name: hadoop-config
            configMap:
              name: hadoop-clowder-config
              defaultMode: 420
        volumeMounts:
          - name: presto-etc
            mountPath: /opt/presto/presto-server/etc
          - name: presto-common-config
            mountPath: /presto-common
          - name: presto-catalog-config
            mountPath: /presto-etc/catalog
          - name: presto-jmx-config
            mountPath: /opt/jmx_exporter/config
          - name: presto-data
            mountPath: /var/presto/data
          - name: presto-logs
            mountPath: /var/presto/logs
          - name: presto-worker-config
            mountPath: /presto-etc
          - name: hadoop-config
            mountPath: /hadoop-config
    objectStore:
    - ${S3_BUCKET_NAME}
    database:
      sharedDbAppName: koku
    dependencies:
      - koku


parameters:
- description: Initial amount of memory the Django container will request.
  displayName: Memory Request
  name: COORDINATOR_MEMORY_REQUEST
  required: true
  value: 2Gi
- description: Maximum amount of memory the Django container can use.
  displayName: Memory Limit
  name: COORDINATOR_MEMORY_LIMIT
  required: true
  value: 4Gi
- description: Initial amount of cpu the Django container will request.
  displayName: CPU Request
  name: COORDINATOR_CPU_REQUEST
  required: true
  value: 200m
- description: Maximum amount of cpu the Django container can use.
  displayName: CPU Limit
  name: COORDINATOR_CPU_LIMIT
  required: true
  value: 500m
- description: Initial amount of memory the Django container will request.
  displayName: Memory Request
  name: WORKER_MEMORY_REQUEST
  required: true
  value: 2Gi
- description: Maximum amount of memory the Django container can use.
  displayName: Memory Limit
  name: WORKER_MEMORY_LIMIT
  required: true
  value: 4Gi
- description: Initial amount of cpu the Django container will request.
  displayName: CPU Request
  name: WORKER_CPU_REQUEST
  required: true
  value: 500m
- description: Maximum amount of cpu the Django container can use.
  displayName: CPU Limit
  name: WORKER_CPU_LIMIT
  required: true
  value: '1'
- description: Number of replicas for the coordinator
  displayName: Coordinator replica count
  name: COORDINATOR_REPLICAS
  required: true
  value: '1'
- description: Number of replicas for the worker
  displayName: Worker replica count
  name: WORKER_REPLICAS
  required: true
  value: '3'
- description: Image name
  name: IMAGE
  value: quay.io/cloudservices/ubi-trino
  required: true
- description: Image tag
  displayName: Image tag
  name: IMAGE_TAG
  value: '348-001'
  required: true
- description: Max Memory for a query
  displayName: query.max-memory
  name: QUERY_MAX_MEMORY
  value: '8GB'
  required: true
- description: Max Memory per node for a query
  displayName: query.max-memory-per-node
  name: QUERY_MAX_MEMORY_PER_NODE
  value: '1GB'
  required: true
- description: Max Total Memory for a query
  displayName: query.max-total-memory-per-node
  name: QUERY_MAX_TOTAL_MEMORY_PER_NODE
  value: '2GB'
  required: true
- name: ENV_NAME
  required: false
- name: S3_BUCKET_NAME
  value: 'hccm-s3'
